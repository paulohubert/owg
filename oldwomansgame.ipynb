{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizagem por reforço: jogo da velha\n",
    "\n",
    " Número de posições possíveis com pelo menos duas casas livres = $3^7 \\times 72 = 157.464$ (número de possibilidades para um tabuleiro com $7$ casas, e $3$ possibilidades por casa - O, X e vazio - vezes o número de maneiras possíveis de escolher as duas casas livres)\n",
    " \n",
    "## Método $\\epsilon$-greedy\n",
    "\n",
    "Confrontado com uma posição do tabuleiro, o robô escolhe o movimento com maior recompensa estimada até o momento. Para permitir alguma exploração das possibilidades, com probabilidade $\\epsilon$ o robô escolhe uma posição aleatoriamente.\n",
    "\n",
    "Esta estratégia é implementada pelo jogador *epsilon_edson*\n",
    " \n",
    "## Modelo Beta-Binomial\n",
    "\n",
    "Para uma certa posição $x$, com $k(x)$ casas livres, tenho $n(x)$ ações possíveis. A partir de agora, vou ignorar o $x$, mantendo sempre em mente que teremos um modelo desses abaixo para cada uma das $157.464$ posições.\n",
    "\n",
    "Para cada uma das $k$ ações possíveis, defino uma probabilidade $\\pi(i)$: a probabilidade de vitória caso o jogador escolha a a ação $i$ ($i=1,...k$)\n",
    "\n",
    "Observo $n_i$ jogos começando da mesma posição, e escolho sempre a ação $i$. Conto o número de vitórias $v_i$. Repito isso para todas as $k$ posições; ou seja, observo $(v_i, n_i)$ para $i=1,...,k$.  \n",
    "\n",
    "Supondo que conheço as probabilidades $\\pi(i)$, a probabilidade associada ao número de jogos e vitórias observadas (a verossimilhança) é uma binomial:\n",
    "\n",
    "$P(v_i | n_i, \\pi(i)) \\propto \\pi(i)^{v_i}(1-\\pi(i))^{n_i-v_i}$\n",
    "\n",
    "Considerando agora todas as posições, e supondo que os resultados dos jogos são independentes (condicionalmente a $\\pi$), temos a verossimilhança conjunta de todas as posições:\n",
    "\n",
    "$P(\\mathbf{v} | \\mathbf{n}, \\mathbf{\\pi}) \\propto \\prod_{i=1}^k \\pi(i)^{v_i}(1-\\pi(i))^{n_i-v_i}$\n",
    "\n",
    "Como não conheço $\\pi$, vou modelar minha ignorância usando uma distribuição de probabilidade para esses valores (uma priori). Por questões matemáticas, escolho uma distribuição Beta para cada possível ação:\n",
    "\n",
    "$P(\\pi(i)) \\propto \\pi(i)^{\\alpha_i - 1}(1-\\pi(i))^{\\beta_i-1}$\n",
    "\n",
    "Onde $\\alpha_i$ e $\\beta_i$ são hiperparâmetros que vamos precisar escolher (em lugar de aprender seus valores). \n",
    "\n",
    "### Aprendizagem e o teorema de Bayes\n",
    "\n",
    "Eu começo então dizendo que a probabilidade de vitória do movimento $i$ tem distribuição a priori Beta com hiperparâmetros $\\alpha_i$ e $\\beta_i$; jogo uma vez e escolho o movimento $i$. Como incorporo essa nova informação?\n",
    "\n",
    "Pelo teorema de Bayes, e usando a propriedade de conjugação entre priori Beta e verossimilhança binomial, sei que a nova distribuição de $\\pi(i)$ é de novo uma Beta, mas agora com parâmetros $(\\alpha_i + 1, \\beta_i)$ em caso de vitória, e $(\\alpha_i, \\beta_i+1)$ em caso de derrota. \n",
    "\n",
    "Portanto a equação de aprendizagem desse modelo, para cada posição $x$ e cada possível movimento $i_x$, é:\n",
    "\n",
    "$\\begin{align}\n",
    "&\\alpha_i(t+1) = \\alpha_i(t) + r_i(t) \\\\\n",
    "&\\beta_i(t+1) = \\beta_i(t) + (1-r_i(t))\n",
    "\\end{align}$\n",
    "\n",
    "onde $r_i(t) = 1$ no caso de vitória, e $0$ caso contrário.\n",
    "\n",
    "Observação: agora temos uma interpretação para os hiperparâmetros $\\alpha$ e $\\beta$; eles representam quantas vitórias e quantas derrotas o robô viu até o momento em que começa a nova rodada de aprendizagem.\n",
    "\n",
    "### Decisão\n",
    "\n",
    "O robô chegou numa posição $x$ com diversas ações possíveis. Ele tem uma distribuição de probabilidade para a probabilidade de vitória (a probabilidade de uma probabilidade...) de cada lance. O que fazer?\n",
    "\n",
    "1. Olhar o valor esperado de probabilidade de vitória para cada movimento; escolher o movimento com maior valor esperado. Esta estratégia é implementada pelo jogador *cientista_sovina*\n",
    "2. Sortear uma probabilidade de vitória para cada movimento, conforme a distribuição atual; escolher o movimento com maior valor sorteado. Esta estratégia é implementada pelo jogador *cientista*\n",
    "\n",
    "## Look-ahead\n",
    "\n",
    "As estratégias acima se baseiam somente na aprendizagem pura para obter uma política de jogo. Uma possibilidade de acelerar o aprendizado é incluir um *lookahead*, ou seja, avaliar o tabuleiro alguns passos adiante.\n",
    "\n",
    "O jogador *miope* implementa essa estratégia de forma única: joga aleatoriamente, a menos que no tabuleiro atual haja um movimento vencedor para qualquer um dos lados. Neste caso, ele joga o movimento vencedor (para ganhar o jogo ou impedir que o adversário ganhe).\n",
    "\n",
    "O jogador *cientista_cauteloso* inclui esse lookahead na estratégia do cientista.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "O jogador *cientista_conciliador* implementa a mesma política de aprendizagem do *cientista*, mas com uma função de recompensa diferente: este jogador quer aprender como **empatar** o jogo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "from owg_board import owg\n",
    "from owg_players import jb, miope, cientista, cientista_sovina, cientista_cauteloso, cientista_conciliador, epsilon_edson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa jogadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = epsilon_edson(desconto = 0.9, epsilon = 0.5, nome = 'epsilon_edson_1MM')\n",
    "\n",
    "p2 = epsilon_edson(desconto = 0.9, epsilon = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reseta o tabuleiro (desnecessário no primeiro round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.reset()\n",
    "p2.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino por self-learning\n",
    "\n",
    "p1 joga contra p2 por $n$ jogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de jogos para treinar\n",
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 636.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Contadores\n",
    "vitorias = 0\n",
    "derrotas = 0\n",
    "empates = 0\n",
    "\n",
    "# Proporções de cada resultado ao longo do tempo\n",
    "pempate = []\n",
    "pjog1 = []\n",
    "pjog2 = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    \n",
    "    # Alterna quem começa o jogo\n",
    "    if i % 2 == 0:\n",
    "        # Joga p1\n",
    "        movimento = p1.joga()\n",
    "        # Comunica o movimento para p2\n",
    "        p2.comunica(movimento)\n",
    "        # Checa se o jogo acabou\n",
    "        res, _ = p2.board.check_result()\n",
    "        while res is None:\n",
    "            # Enquanto não acabar\n",
    "            movimento = p2.joga()\n",
    "            if movimento is not None:\n",
    "                p1.comunica(movimento)\n",
    "                movimento = p1.joga()           \n",
    "                if movimento is not None:\n",
    "                    p2.comunica(movimento)\n",
    "            res, _ = p2.board.check_result()\n",
    "        \n",
    "        # Checa o resultado e atualiza os contadores\n",
    "        r, _ = p1.board.check_result()\n",
    "        if r == 1:\n",
    "            vitorias += 1\n",
    "        elif r == -1:\n",
    "            derrotas += 1\n",
    "        else:\n",
    "            empates += 1\n",
    "        p1.reset()\n",
    "        p2.reset()\n",
    "    else:\n",
    "        # Joga p2\n",
    "        movimento = p2.joga()\n",
    "        # Comunica o movimento para p1\n",
    "        p1.comunica(movimento)\n",
    "        # Checa se o jogo acabou\n",
    "        res, _ = p1.board.check_result()\n",
    "        while res is None:\n",
    "            # Enquanto não acabar\n",
    "            movimento = p1.joga()\n",
    "            if movimento is not None:\n",
    "                p2.comunica(movimento)\n",
    "                movimento = p2.joga()          \n",
    "                if movimento is not None:\n",
    "                    p1.comunica(movimento)\n",
    "            res, _ = p1.board.check_result()\n",
    "        \n",
    "        # Checa o resultado final e atualiza contadores\n",
    "        r, _ = p1.board.check_result()\n",
    "        if r == 1:\n",
    "            vitorias += 1\n",
    "        elif r == -1:\n",
    "            derrotas += 1\n",
    "        else:\n",
    "            empates += 1\n",
    "        p1.reset()\n",
    "        p2.reset()\n",
    "        \n",
    "    pempate.append(empates / (i+1))\n",
    "    pjog1.append(vitorias / (i+1))\n",
    "    pjog2.append(derrotas / (i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico da evoluçao das proporções de empate e vitória de cada jogador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(pjog1, '-', label = p1.nome)\n",
    "plt.plot(pempate, '-', label = 'Empate')\n",
    "plt.plot(pjog2, '-', label = p2.nome)\n",
    "plt.ylim([0,1])\n",
    "plt.hlines(y = 0.5, xmin = 0, xmax = len(pjog1) , linestyles='dashed')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os jogadores pré-treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cientista_1MM.pkl', 'rb') as arq:\n",
    "    p1 = pickle.load(arq)\n",
    "with open('cientista_cauteloso_1MM.pkl', 'rb') as arq:\n",
    "    p2 = pickle.load(arq)\n",
    "with open('epsilon_edson_1MM.pkl', 'rb') as arq:\n",
    "    p3 = pickle.load(arq)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando aprendizagem do jogador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = owg()\n",
    "tab.start_free(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jogar contra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jogar contra quem?\n",
    "p = p1\n",
    "p.reset()\n",
    "tab = owg()\n",
    "tab.start(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
